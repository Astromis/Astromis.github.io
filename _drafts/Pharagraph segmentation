Some time ago when I actively dive into NLP, I learned about token segmentation and sentence segmentation. Even made a toy paper in which I compare several classification algorithms for sentence boundary detection (achived that algorithms has F1 score about 99.99). Moving ahead I asked is there an algorithm to segment a paragraphs? Agree, that it is more uncertain task as previous one because there is no strong rules how to divide text into paragraphs. Let me borrow a paragraph definition from Wikipedia - paragraph is a self-contained unit of a discourse in writing dealing with a particular point or idea.
Why are paragraphs important? So it is comfortably to read the text separated by discrete ideas because you can stop at the end of each one and think about what you have just read. Much more hard to do the same with plain text, for me at least. Imagine you need to compile a survey from several sources. It is defenetly easier to pick up the main topics by paragraphs and then reconstruct it to the new text. I hope you agree with me that paragraphs are important for reading. 
In NLP it is higher tier of text representation that can be formulated as idea unit level. You can represent your text as idea set and, for example, search though these set a particular idea or to cluster the text corpus by idea. But question stands the same - how to split the text into coherent paragraphs? I stated tp search what have been already done. [The oldest paper](https://www.aclweb.org/anthology/J97-1003) describes a TextTiling Algorithm that uses so-called lexical chains to divede expository texts. For me, it says that algorithm bases on lingustic knowladge so it cannot be effective method, that was approved by experiments some later. In two words, the algorithms consists of three parts: tokenization, similarity measure amd boundary detection. You split your text on fixed word size blocks, measure the similarity between them and point the boundary where it chooses hard. This algorthim is included in NLTK so we can play with it.

[playing with nltk]

Sure there are papers that represents this problem as supervised task, [this](https://arxiv.org/pdf/1803.09337.pdf) for exapmle, but I was interesting in unsupervised algorithm based on word embeddings. I was lucky and found, you never guess how what the name is - [Text Segmentation based on Semantic Word Embeddings](https://arxiv.org/pdf/1503.05543.pdf). From this paper I had known that Choi contributed a lot of work in this topic such as special corpus for this task and C99 algorithm, time by time the paper in this topic appears and someone tried to apply the word2vec before this work. So let's go to the algorithm.
